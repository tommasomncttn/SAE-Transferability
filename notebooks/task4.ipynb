{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LVX-XSoCaj6D",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "a2a989bf-b56f-4ea5-8ffc-41d3ec04257a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sae-lens in /usr/local/lib/python3.10/dist-packages (3.22.2)\n",
            "Requirement already satisfied: transformer-lens in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
            "Requirement already satisfied: torcheval in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: automated-interpretability<1.0.0,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.0.6)\n",
            "Requirement already satisfied: babe<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.0.7)\n",
            "Requirement already satisfied: datasets<3.0.0,>=2.17.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (2.21.0)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (3.9.2)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.1.7)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (3.8.1)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.19.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (5.24.1)\n",
            "Requirement already satisfied: plotly-express<0.5.0,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.4.1)\n",
            "Requirement already satisfied: pytest-profiling<2.0.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (1.7.0)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (1.0.1)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (6.0.2)\n",
            "Requirement already satisfied: pyzmq==26.0.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (26.0.0)\n",
            "Requirement already satisfied: safetensors<0.5.0,>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.4.5)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (4.44.2)\n",
            "Requirement already satisfied: typer<0.13.0,>=0.12.3 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (4.12.2)\n",
            "Requirement already satisfied: zstandard<0.23.0,>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.22.0)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.34.2)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.0.3)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.8.0)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.2.34)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (2.1.4)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (13.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (4.66.5)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.18.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens) (0.24.7)\n",
            "Requirement already satisfied: blobfile<3.0.0,>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae-lens) (2.1.1)\n",
            "Requirement already satisfied: boostedblob<0.16.0,>=0.15.3 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae-lens) (0.15.4)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae-lens) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae-lens) (3.10.7)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae-lens) (1.5.2)\n",
            "Requirement already satisfied: tiktoken<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae-lens) (0.6.0)\n",
            "Requirement already satisfied: py2store in /usr/local/lib/python3.10/dist-packages (from babe<0.0.8,>=0.0.7->sae-lens) (0.1.20)\n",
            "Requirement already satisfied: graze in /usr/local/lib/python3.10/dist-packages (from babe<0.0.8,>=0.0.7->sae-lens) (0.1.24)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<3.0.0,>=2.17.1->sae-lens) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (3.10.5)\n",
            "Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer-lens) (2.13.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (1.4.7)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (2.8.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from matplotlib-inline<0.2.0,>=0.1.6->sae-lens) (5.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->sae-lens) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->sae-lens) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->sae-lens) (2024.9.11)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens) (2024.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly<6.0.0,>=5.19.0->sae-lens) (9.0.0)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from plotly-express<0.5.0,>=0.4.1->sae-lens) (0.14.3)\n",
            "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.10/dist-packages (from plotly-express<0.5.0,>=0.4.1->sae-lens) (1.13.1)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.10/dist-packages (from plotly-express<0.5.0,>=0.4.1->sae-lens) (0.5.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pytest-profiling<2.0.0,>=1.7.0->sae-lens) (1.16.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from pytest-profiling<2.0.0,>=1.7.0->sae-lens) (7.4.4)\n",
            "Requirement already satisfied: gprof2dot in /usr/local/lib/python3.10/dist-packages (from pytest-profiling<2.0.0,>=1.7.0->sae-lens) (2024.6.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens) (2.18.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens) (3.1.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.1->sae-lens) (0.19.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.13.0,>=0.12.3->sae-lens) (1.5.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (3.20.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (2.14.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens) (71.0.4)\n",
            "Requirement already satisfied: pycryptodomex~=3.8 in /usr/local/lib/python3.10/dist-packages (from blobfile<3.0.0,>=2.1.1->automated-interpretability<1.0.0,>=0.0.5->sae-lens) (3.20.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from blobfile<3.0.0,>=2.1.1->automated-interpretability<1.0.0,>=0.0.5->sae-lens) (2.2.3)\n",
            "Requirement already satisfied: lxml~=4.9 in /usr/local/lib/python3.10/dist-packages (from blobfile<3.0.0,>=2.1.1->automated-interpretability<1.0.0,>=0.0.5->sae-lens) (4.9.4)\n",
            "Requirement already satisfied: uvloop>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from boostedblob<0.16.0,>=0.15.3->automated-interpretability<1.0.0,>=0.0.5->sae-lens) (0.20.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.17.1->sae-lens) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens) (4.0.11)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<1.0.0,>=0.0.5->sae-lens) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<1.0.0,>=0.0.5->sae-lens) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<1.0.0,>=0.0.5->sae-lens) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<1.0.0,>=0.0.5->sae-lens) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<1.0.0,>=0.0.5->sae-lens) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->automated-interpretability<1.0.0,>=0.0.5->sae-lens) (0.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<3.0.0,>=2.17.1->sae-lens) (3.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.0->automated-interpretability<1.0.0,>=0.0.5->sae-lens) (3.5.0)\n",
            "Requirement already satisfied: dol in /usr/local/lib/python3.10/dist-packages (from graze->babe<0.0.8,>=0.0.7->sae-lens) (0.2.76)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer-lens) (2.1.5)\n",
            "Requirement already satisfied: config2py in /usr/local/lib/python3.10/dist-packages (from py2store->babe<0.0.8,>=0.0.7->sae-lens) (0.1.36)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from py2store->babe<0.0.8,>=0.0.7->sae-lens) (6.4.5)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->pytest-profiling<2.0.0,>=1.7.0->sae-lens) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->pytest-profiling<2.0.0,>=1.7.0->sae-lens) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->pytest-profiling<2.0.0,>=1.7.0->sae-lens) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pytest-profiling<2.0.0,>=1.7.0->sae-lens) (2.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer-lens) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens) (5.0.1)\n",
            "Requirement already satisfied: i2 in /usr/local/lib/python3.10/dist-packages (from config2py->py2store->babe<0.0.8,>=0.0.7->sae-lens) (0.1.33)\n"
          ]
        }
      ],
      "source": [
        "%pip install sae-lens transformer-lens torcheval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb4s_ZaaaYqZ",
        "outputId": "41f564b8-74f7-4df0-d958-832c4518722c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Standard imports\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import einops\n",
        "\n",
        "# import the LLM\n",
        "from sae_lens import SAE, HookedSAETransformer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# For the most part I'll try to import functions and classes near where they are used\n",
        "# to make it clear where they come from.\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# utility to clear variables out of the memory & and clearing cuda cache\n",
        "import gc\n",
        "def clear_cache():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notebook config"
      ],
      "metadata": {
        "id": "K13oQfzQ7sol"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1-2VqKZaYqa"
      },
      "outputs": [],
      "source": [
        "# define the model to work with\n",
        "MODEL = 'MISTRAL' # GEMMA, GPT2\n",
        "\n",
        "if MODEL == 'GEMMA':\n",
        "    RELEASE = 'gemma-2b-res-jb'\n",
        "    BASE_MODEL = \"google/gemma-2b\"\n",
        "    FINETUNE_MODEL = 'shahdishank/gemma-2b-it-finetune-python-codes'\n",
        "    DATASET_NAME = \"ctigges/openwebtext-gemma-1024-cl\"\n",
        "    FINETUNE_PATH = None\n",
        "    BASE_TOKENIZER_NAME = BASE_MODEL\n",
        "\n",
        "    hook_part = 'post'\n",
        "    layer_num = 6\n",
        "elif MODEL == 'GPT2':\n",
        "    RELEASE = 'gpt2-small-res-jb'\n",
        "    BASE_MODEL = \"gpt2-small\"\n",
        "    FINETUNE_MODEL = 'pierreguillou/gpt2-small-portuguese'\n",
        "    FINETUNE_PATH = None\n",
        "    DATASET_NAME = \"Skylion007/openwebtext\"\n",
        "    BASE_TOKENIZER_NAME = BASE_MODEL\n",
        "\n",
        "    hook_part = 'pre'\n",
        "    layer_num = 6\n",
        "elif MODEL == 'MISTRAL':\n",
        "    RELEASE = 'mistral-7b-res-wg'\n",
        "    BASE_MODEL = \"mistral-7b\"\n",
        "    DATASET_NAME = \"monology/pile-uncopyrighted\"\n",
        "    BASE_TOKENIZER_NAME = 'mistralai/Mistral-7B-v0.1'\n",
        "\n",
        "    FINETUNE_MODEL = 'meta-math/MetaMath-Mistral-7B' #DeepMount00/Mistral-Ita-7b\n",
        "    FINETUNE_PATH = f'/content/drive/My Drive/Finetunes/MetaMath-Mistral-7B'\n",
        "\n",
        "    hook_part = 'pre'\n",
        "    layer_num = 8\n",
        "\n",
        "SAE_HOOK = f'blocks.{layer_num}.hook_resid_{hook_part}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F91e8uc7P4en"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "class Experiment(Enum):\n",
        "    SUBSTITUTION_LOSS = 'SubstitutionLoss'\n",
        "    L0_LOSS = 'L0_loss'\n",
        "    FEATURE_ACTS = 'FeatureActs'\n",
        "    FEATURE_DENSITY = 'FeatureDensity'\n",
        "\n",
        "TOTAL_BATCHES = {\n",
        "    Experiment.SUBSTITUTION_LOSS: 50,\n",
        "    Experiment.L0_LOSS: 50,\n",
        "    Experiment.FEATURE_ACTS: 10,\n",
        "    Experiment.FEATURE_DENSITY: 50\n",
        "}\n",
        "\n",
        "TOKENS_SAMPLE = {\n",
        "    Experiment.SUBSTITUTION_LOSS: [],\n",
        "    Experiment.L0_LOSS: [],\n",
        "    Experiment.FEATURE_ACTS: [],\n",
        "    Experiment.FEATURE_DENSITY: []\n",
        "}\n",
        "\n",
        "def get_batch_size(key: Experiment):\n",
        "    return TOTAL_BATCHES[key]\n",
        "\n",
        "def get_tokens_sample(key: Experiment):\n",
        "    return TOKENS_SAMPLE[key]\n",
        "\n",
        "def set_tokens_sample(key: Experiment, token_sample):\n",
        "    TOKENS_SAMPLE[key] = token_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jif4TBN7BL_K"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading finetune model"
      ],
      "metadata": {
        "id": "nBwWGuIZ4e6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def adjust_state_dict(model, base_model_vocab_size):\n",
        "    \"\"\"Adjust the state_dict of the model to match the base model's vocab size.\"\"\"\n",
        "    state_dict = model.state_dict()\n",
        "\n",
        "    # Adjust the embedding matrix\n",
        "    if state_dict['model.embed_tokens.weight'].shape[0] > base_model_vocab_size:\n",
        "        state_dict['model.embed_tokens.weight'] = state_dict['model.embed_tokens.weight'][:base_model_vocab_size, :]\n",
        "\n",
        "    # Adjust the unembedding (lm_head) matrix\n",
        "    if state_dict['lm_head.weight'].shape[0] > base_model_vocab_size:\n",
        "        state_dict['lm_head.weight'] = state_dict['lm_head.weight'][:base_model_vocab_size, :]\n",
        "\n",
        "    return state_dict\n",
        "\n",
        "def load_hf_model(path, base_model=BASE_MODEL, device='cuda', dtype=None):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(path)\n",
        "\n",
        "    # Adjust the model's state dict to match the base model's vocab size\n",
        "    if base_model == 'mistral-7b':\n",
        "      base_model_vocab_size = 32000  # Mistral 7B base vocab size\n",
        "      adjusted_state_dict = adjust_state_dict(model, base_model_vocab_size)\n",
        "\n",
        "      # Adjust model architecture to match the new vocab size\n",
        "      model.resize_token_embeddings(base_model_vocab_size)\n",
        "\n",
        "      # Load the adjusted state dict back into the model\n",
        "      model.load_state_dict(adjusted_state_dict, strict=False)\n",
        "\n",
        "    # Now load the fine-tuned model into the HookedSAETransformer\n",
        "    finetune_model = HookedSAETransformer.from_pretrained(\n",
        "        base_model, device=device, hf_model=model, dtype=dtype\n",
        "    )\n",
        "\n",
        "    del model  # offload the HF model as it's already wrapped into HookedSAETransformer (finetune_model)\n",
        "    clear_cache()\n",
        "\n",
        "    return tokenizer, finetune_model"
      ],
      "metadata": {
        "id": "M4N-rtoy4iHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1FcS0z1BL_L"
      },
      "source": [
        "#### Activations filtering utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoRMmhbhBL_L"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "import os\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "def load_outliers_config(filename='outlier_cfg.json'):\n",
        "    \"\"\"\n",
        "    This function checks if the script is running in Google Colab and loads the JSON file accordingly.\n",
        "    If running in Colab, it will mount Google Drive and load the file from there.\n",
        "    Otherwise, it will load the file from a local directory.\n",
        "    \"\"\"\n",
        "    if not IN_COLAB:\n",
        "        # If not in Colab, use local folder\n",
        "        # Assuming this is being run from the 'notebooks' folder\n",
        "        sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
        "\n",
        "        from saetuning.utils import OUTLIERS_CFG\n",
        "        return OUTLIERS_CFG\n",
        "\n",
        "    # If in Colab, mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Define the path to your JSON file in Google Drive\n",
        "    file_path = os.path.join('/content/drive/My Drive', filename)\n",
        "    print(f\"Loading JSON file from Google Drive: {file_path}\")\n",
        "\n",
        "    # Load the JSON data\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI6JqtBVBL_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a35ed334-e0a3-4e11-e9b5-cf8b2f3007cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading JSON file from Google Drive: /content/drive/My Drive/outlier_cfg.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'norm_scalar': {'google/gemma-2b': 0.31278620989943556,\n",
              "  'gpt2-small': 0.27139524668485193,\n",
              "  'mistral-7b': 14.178454680291779},\n",
              " 'threshhold_multiplier': {'google/gemma-2b': 2, 'gpt2-small': 2},\n",
              " 'base_threshhold': {'google/gemma-2b': 45.254833995939045,\n",
              "  'gpt2-small': 27.712812921102035},\n",
              " 'absolute_threshold': {'mistral-7b': 200}}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "OUTLIERS_CFG = load_outliers_config()\n",
        "\n",
        "def get_norm_scalar(model_name):\n",
        "    return OUTLIERS_CFG.get(\"norm_scalar\", {}).get(model_name, None)\n",
        "\n",
        "def get_threshold_multiplier(model_name):\n",
        "    return OUTLIERS_CFG.get(\"threshhold_multiplier\", {}).get(model_name, None)\n",
        "\n",
        "def get_base_threshhold(model_name):\n",
        "    return OUTLIERS_CFG.get(\"base_threshhold\", {}).get(model_name, None)\n",
        "\n",
        "def get_absolute_threshhold(model_name):\n",
        "    return OUTLIERS_CFG.get(\"absolute_threshold\", {}).get(model_name, None)\n",
        "\n",
        "# Auxilary method for getting a mask of outlier activations\n",
        "def is_act_outlier(act_tensor, model_name):\n",
        "    \"\"\"\n",
        "    Expects act_tensor of shape [*, D_MODEL]\n",
        "\n",
        "    Returns a boolean tensor of shape [*], where for each batch position we report whether the corresponding activation\n",
        "    exceeds the outlier threshold that is defined as\n",
        "\n",
        "    threshold = threshold_multiplier * base_threshold, where\n",
        "    base_threshold = sqrt(D_MODEL)\n",
        "\n",
        "    Important! This threshold value is in the normalized scale, i.e. is meant to be used for activations that are scaled\n",
        "    in such a way, that their average norm is equal to sqrt(D_MODEL). To do this normalization, we multiple by norm_scalar\n",
        "    of the corresponding model.\n",
        "\n",
        "    Check this blog-post for more details: https://www.lesswrong.com/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models\n",
        "    \"\"\"\n",
        "    norm_scalar = get_norm_scalar(model_name)\n",
        "    threshold_multiplier = get_threshold_multiplier(model_name)\n",
        "    base_threshold = get_base_threshhold(model_name)\n",
        "    absolute_threshhold = get_absolute_threshhold(model_name)\n",
        "\n",
        "    if absolute_threshhold:\n",
        "        threshold = norm_scalar * absolute_threshhold\n",
        "    else:\n",
        "        threshold = threshold_multiplier * base_threshold\n",
        "\n",
        "    scaled_act = norm_scalar * act_tensor\n",
        "    scaled_act_norms = torch.norm(scaled_act, dim=-1)\n",
        "\n",
        "    return scaled_act_norms > threshold\n",
        "\n",
        "OUTLIERS_CFG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01fn9LiFBL_M"
      },
      "outputs": [],
      "source": [
        "def filter_activations(acts, model_name=BASE_MODEL, return_mask=False):\n",
        "    \"\"\"\n",
        "    Filters out activations based on outlier norms and returns the filtered activations.\n",
        "\n",
        "    Args:\n",
        "        acts (torch.Tensor): A tensor of activations with shape [BATCH, SEQ, D_MODEL].\n",
        "        model_name (str): The name of the model used to determine the threshold for filtering out outlier activations.\n",
        "        return_mask (bool): If True, returns the 2D boolean mask indicating which activations were retained. The mask has shape [BATCH, SEQ].\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A tensor of filtered activations with shape [N_VALID_ACTIVATIONS, D_MODEL], where N_VALID_ACTIVATIONS <= BATCH * SEQ.\n",
        "        torch.Tensor (optional): A 2D boolean tensor of shape [BATCH, SEQ] representing the filtering mask, indicating whether each activation was retained (True) or filtered out (False).\n",
        "\n",
        "    Notes:\n",
        "        - The function removes activations identified as outliers by `is_act_outlier`. The activations that pass the filter are flattened into a tensor of shape [N_VALID_ACTIVATIONS, D_MODEL].\n",
        "        - If `return_mask=True`, the function also returns a 2D boolean mask corresponding to the [BATCH, SEQ] dimensions of the original activations. This mask can be useful for tracking which activations were kept.\n",
        "        - The returned filtered activations are flattened across both batch and sequence dimensions. If reshaping back to a sequence or batch structure is required, you will need to do this outside the function based on the original mask.\n",
        "    \"\"\"\n",
        "    # Get the outlier mask\n",
        "    is_outlier_mask = is_act_outlier(acts, model_name)  # [BATCH, SEQ]\n",
        "\n",
        "    # Expand the mask to match the last dimension (D_MODEL) for correct filtering\n",
        "    expanded_mask = is_outlier_mask.unsqueeze(-1).expand_as(acts)  # [BATCH, SEQ, D_MODEL]\n",
        "\n",
        "    # Apply the mask and filter out the outlier activations\n",
        "    filtered_acts = acts[~expanded_mask].reshape(-1, acts.shape[-1])  # Flatten only the valid activations, retaining D_MODEL\n",
        "\n",
        "    if return_mask:\n",
        "        # Return the 2D mask corresponding to the original [BATCH, SEQ] shape\n",
        "        filter_mask = ~is_outlier_mask  # Keep it as 2D: [BATCH, SEQ]\n",
        "        return filtered_acts, filter_mask\n",
        "    else:\n",
        "        return filtered_acts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MVm963cBL_M"
      },
      "source": [
        "#### Score functions definition (copy from saetuning/utils.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJYK_EofaYqb"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from enum import Enum\n",
        "import numpy as np\n",
        "from scipy.stats import gamma\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from the .env file\n",
        "load_dotenv()\n",
        "\n",
        "#### Quantitave SAE evaluation ####\n",
        "def L0_loss(x, threshold=1e-8):\n",
        "    \"\"\"\n",
        "    Expects a tensor x of shape [N_TOKENS, N_SAE].\n",
        "\n",
        "    Returns a scalar representing the mean value of activated features (i.e. values across the N_SAE dimensions bigger than\n",
        "    the threshhold), a.k.a. L0 loss.\n",
        "    \"\"\"\n",
        "    return (x > threshold).float().sum(-1).mean()\n",
        "\n",
        "def get_substitution_loss(tokens, model, sae, sae_layer, reconstruction_metric=None):\n",
        "    '''\n",
        "    Expects a tensor of input tokens of shape [N_BATCHES, N_CONTEXT].\n",
        "\n",
        "    Returns two losses:\n",
        "    1. Clean loss - loss of the normal forward pass of the model at the input tokens.\n",
        "    2. Substitution loss - loss when substituting SAE reconstructions of the residual stream at the SAE layer of the model.\n",
        "    '''\n",
        "    # Run the model with cache to get the original activations and clean loss\n",
        "    loss_clean, cache = model.run_with_cache(tokens, names_filter=[sae_layer], return_type=\"loss\")\n",
        "\n",
        "    # Fetch and detach the original activations\n",
        "    original_activations = cache[sae_layer]\n",
        "\n",
        "    # Apply activation filtering\n",
        "    activations_filtered, filter_mask = filter_activations(original_activations, return_mask=True)\n",
        "    # Shape of activations_filtered is now [valid_activations, d_model]\n",
        "\n",
        "    # Get the SAE reconstructed activations\n",
        "    post_reconstructed = sae.forward(activations_filtered)# shape [valid_activations, d_model]\n",
        "\n",
        "    # Update the reconstruction quality metric, if provided\n",
        "    if reconstruction_metric:\n",
        "        reconstruction_metric.update(post_reconstructed.flatten().float(), activations_filtered.flatten().float())\n",
        "\n",
        "    # Free unused variables early to save memory\n",
        "    del original_activations, activations_filtered, cache\n",
        "    clear_cache()\n",
        "\n",
        "    # Modified hook function\n",
        "    def hook_function(activations, hook, new_activations, filter_mask):\n",
        "        # activations: [batch_size, seq_len, d_model]\n",
        "        # filter_mask: [batch_size, seq_len]\n",
        "        # new_activations: [valid_activations, d_model]\n",
        "\n",
        "        # Flatten activations and filter_mask\n",
        "        activations_flat = activations.view(-1, activations.shape[-1])\n",
        "        filter_mask_flat = filter_mask.view(-1)\n",
        "\n",
        "        # Replace activations at positions specified by filter_mask\n",
        "        activations_flat[filter_mask_flat] = new_activations\n",
        "\n",
        "        # Reshape back to original shape\n",
        "        activations = activations_flat.view(activations.shape)\n",
        "\n",
        "        return activations\n",
        "\n",
        "    post_reconstructed = post_reconstructed.half() # Reduce to fp16 because we'll splice it in to the model\n",
        "\n",
        "    # Run the model again with hooks to substitute activations at the SAE layer\n",
        "    loss_reconstructed = model.run_with_hooks(\n",
        "        tokens,\n",
        "        return_type=\"loss\",\n",
        "        fwd_hooks=[(sae_layer, partial(hook_function, new_activations=post_reconstructed, filter_mask=filter_mask))]\n",
        "    )\n",
        "\n",
        "    # Clean up the reconstructed activations and clear memory\n",
        "    del post_reconstructed\n",
        "    clear_cache()\n",
        "\n",
        "    return loss_clean, loss_reconstructed\n",
        "\n",
        "import plotly.graph_objs as go\n",
        "from functools import partial\n",
        "\n",
        "def plot_log10_hist(y_data, y_value, num_bins=100, first_bin_name = 'First bin value',\n",
        "                    y_scalar=1.5, y_scale_bin=-2, log_epsilon=1e-10):\n",
        "    \"\"\"\n",
        "    Computes the histogram using PyTorch and plots the feature density diagram with log-10 scale using Plotly.\n",
        "    Y-axis is clipped to the value of the second-largest bin to prevent suppression of smaller values.\n",
        "    \"\"\"\n",
        "    # Flatten the tensor\n",
        "    y_data_flat = torch.flatten(y_data)\n",
        "\n",
        "    # Compute the logarithmic transformation using PyTorch\n",
        "    log_y_data_flat = torch.log10(torch.abs(y_data_flat) + log_epsilon).detach().cpu()\n",
        "\n",
        "    # Compute histogram using PyTorch\n",
        "    hist_min = torch.min(log_y_data_flat).item()\n",
        "    hist_max = torch.max(log_y_data_flat).item()\n",
        "    hist_range = hist_max - hist_min\n",
        "    bin_edges = torch.linspace(hist_min, hist_max, num_bins + 1)\n",
        "    hist_counts, _ = torch.histogram(log_y_data_flat, bins=bin_edges)\n",
        "\n",
        "    # Convert data to NumPy for Plotly\n",
        "    bin_edges_np = bin_edges.detach().cpu().numpy()\n",
        "    hist_counts_np = hist_counts.detach().cpu().numpy()\n",
        "\n",
        "    # Find the largest and second-largest bin values\n",
        "    first_bin_value = hist_counts_np[0]\n",
        "    scale_bin_value = sorted(hist_counts_np)[y_scale_bin]  # Get the second largest bin value (by default)\n",
        "\n",
        "    # Prepare the Plotly plot\n",
        "    fig = go.Figure(\n",
        "        data=[go.Bar(\n",
        "            x=bin_edges_np[:-1],  # Exclude the last bin edge\n",
        "            y=hist_counts_np,\n",
        "            width=hist_range / num_bins,\n",
        "        )]\n",
        "    )\n",
        "\n",
        "    # Update the layout for the plot, clipping the y-axis at the second largest bin value\n",
        "    fig.update_layout(\n",
        "        title=f\"SAE Features {y_value} histogram ({first_bin_name}: {first_bin_value:.2e})\",\n",
        "        xaxis_title=f\"Log10 of {y_value}\",\n",
        "        yaxis_title=\"Density\",\n",
        "        yaxis_range=[0, scale_bin_value * y_scalar],  # Clipping to the second-largest value by default\n",
        "        bargap=0.2,\n",
        "        bargroupgap=0.1,\n",
        "    )\n",
        "\n",
        "    # Add an annotation to display the value of the first bin\n",
        "    fig.add_annotation(\n",
        "        text=f\"{first_bin_name}: {first_bin_value:.2e}\",\n",
        "        xref=\"paper\", yref=\"paper\",\n",
        "        x=0.95, y=0.95,\n",
        "        showarrow=False,\n",
        "        font=dict(size=12, color=\"red\"),\n",
        "        bgcolor=\"white\",\n",
        "        bordercolor=\"black\",\n",
        "        borderwidth=1\n",
        "    )\n",
        "\n",
        "    # Show the plot\n",
        "    fig.show()\n",
        "\n",
        "class FeatureDensityPlotter:\n",
        "    def __init__(self, n_features, n_tokens, activation_threshold=1e-10, num_bins=100):\n",
        "        self.num_bins = num_bins\n",
        "        self.activation_threshold = activation_threshold\n",
        "\n",
        "        self.n_tokens = n_tokens\n",
        "        self.n_features = n_features\n",
        "\n",
        "        # Initialize a tensor of feature densities for all features,\n",
        "        # where feature density is defined as the fraction of tokens on which the feature has a nonzero value.\n",
        "        self.feature_densities = torch.zeros(n_features, dtype=torch.float32)\n",
        "\n",
        "    def update(self, feature_acts):\n",
        "        \"\"\"\n",
        "        Expects a tensor feature_acts of shape [N_TOKENS, N_FEATURES].\n",
        "\n",
        "        Updates the feature_densities buffer:\n",
        "        1. For each feature, count the number of tokens that the feature activated on (i.e. had an activation greater than the activation_threshold)\n",
        "        2. Add this count at the feature's position in the feature_densities tensor, divided by the total number of tokens (to compute the fraction)\n",
        "        \"\"\"\n",
        "\n",
        "        activating_tokens_count = (feature_acts > self.activation_threshold).float().sum(0)\n",
        "        self.feature_densities += activating_tokens_count / self.n_tokens\n",
        "\n",
        "    def plot(self, num_bins=100, y_scalar=1.5, y_scale_bin=-2, log_epsilon=1e-10):\n",
        "        plot_log10_hist(self.feature_densities, 'Density', num_bins=num_bins, first_bin_name='Dead features density',\n",
        "                        y_scalar=y_scalar, y_scale_bin=y_scale_bin, log_epsilon=log_epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABlYQE9JaYqb"
      },
      "source": [
        "### Task 4.1 Pretrained case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8oRMG-qaYqb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "2d12eb0ab6ae49a7857b9bd33886edd1",
            "6f390c934f4f4df6b175c00fad546ac4",
            "16f28c24722f4a45989928a2f7b87107",
            "772465b9d19041a196494f0424756f6d",
            "daaf163e20d942968e37a033db88bbe8",
            "e65f57af640f4db9819bb1561579d42f",
            "c66930b0cb43498dbfd9340338ef0ca2",
            "d6d3c329a9b9446ebca217bf4301c04e",
            "44d805d0088d4d2eba4e25d90fa55f6d",
            "35f4620e37ff47d4a540798210439ec0",
            "ee3a337275cb42218137525c046e053a"
          ]
        },
        "outputId": "800b2d89-0b20-4e58-b349-8e6b9aecf765"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d12eb0ab6ae49a7857b9bd33886edd1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
            "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model mistral-7b into HookedTransformer\n"
          ]
        }
      ],
      "source": [
        "base_model = HookedSAETransformer.from_pretrained(BASE_MODEL, device=device, dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbjpczwWaYqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e15c872-d65e-42b0-82b2-291707f951c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'d_in': 4096,\n",
              " 'd_sae': 65536,\n",
              " 'dtype': 'float32',\n",
              " 'device': 'cuda',\n",
              " 'model_name': 'mistral-7b',\n",
              " 'hook_name': 'blocks.8.hook_resid_pre',\n",
              " 'hook_layer': 8,\n",
              " 'hook_head_index': None,\n",
              " 'activation_fn_str': 'relu',\n",
              " 'apply_b_dec_to_input': False,\n",
              " 'finetuning_scaling_factor': False,\n",
              " 'sae_lens_training_version': None,\n",
              " 'prepend_bos': False,\n",
              " 'dataset_path': 'monology/pile-uncopyrighted',\n",
              " 'context_size': 256,\n",
              " 'normalize_activations': 'constant_norm_rescale',\n",
              " 'dataset_trust_remote_code': True,\n",
              " 'architecture': 'standard',\n",
              " 'neuronpedia': None}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# import the required libraries\n",
        "from sae_lens import SAE\n",
        "\n",
        "sae_id = f'blocks.{layer_num}.hook_resid_{hook_part}'\n",
        "\n",
        "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
        "                            release = RELEASE,\n",
        "                            sae_id = sae_id,\n",
        "                            device = device\n",
        ")\n",
        "cfg_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIFp_EXJaYqc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7897b1f8-a3b5-40de-ba25-21c74011ef12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'relu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# this must be checked for the forward method of sae.encode_xxx\n",
        "cfg_dict[\"activation_fn_str\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp9ACCQBaYqc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "a851a7c7f2634655a2f64b0e9de01028",
            "87d2a1cd47ae429c85992ad50b19f410",
            "af7fdc7b78a04aa9990f6de345eb22db",
            "8f94fc386e5d4173a8b90b0d9f396d0e",
            "a67ec4750eb4472dbebf43354af4c58e",
            "ac67f2d4794641e0915469d9df8b8085",
            "54348c8d3d604739a84cb1d31c7a9b88",
            "41ad50fff1b94f2c920ba9c040f0ed5d",
            "e6e20ad247654f9d86fb30c2b304e87d",
            "4632488209fd4ec6a07ea343c0abe900",
            "ea8074c65241405b8adc334684d6ddc2"
          ]
        },
        "outputId": "57f67dab-9c4d-4a45-e6c9-f45e97b203ce"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a851a7c7f2634655a2f64b0e9de01028"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 5120)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from sae_lens import ActivationsStore\n",
        "\n",
        "if MODEL == 'MISTRAL' or MODEL == 'GPT2':\n",
        "    batch_size_prompts = 20\n",
        "else:\n",
        "    batch_size_prompts = 5\n",
        "\n",
        "# a convenient way to instantiate an activation store is to use the from_sae method\n",
        "activation_store = ActivationsStore.from_sae(\n",
        "    model=base_model,\n",
        "    sae=sae,\n",
        "    streaming=True,\n",
        "    # fairly conservative parameters here so can use same for larger\n",
        "    # models without running out of memory.\n",
        "    store_batch_size_prompts=batch_size_prompts,\n",
        "    train_batch_size_tokens=4096,\n",
        "    n_batches_in_buffer=32,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "batch_size_tokens = activation_store.context_size * batch_size_prompts\n",
        "\n",
        "batch_size_prompts, batch_size_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpu0bs7laYqc"
      },
      "source": [
        "#### 4.1.1 L0 loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5b0sdnAaYqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bbe87f3-bd78-4c63-f007-2de212a2d020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 50/50 [00:49<00:00,  1.02it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "all_L0 = []\n",
        "\n",
        "total_batches = get_batch_size(Experiment.L0_LOSS)\n",
        "all_tokens_L0 = get_tokens_sample(Experiment.L0_LOSS)\n",
        "\n",
        "for k in tqdm(range(total_batches)):\n",
        "    # Get a batch of tokens from the dataset\n",
        "    tokens = activation_store.get_batch_tokens()  # [N_BATCH, N_CONTEXT]\n",
        "\n",
        "    # Store tokens for later reuse\n",
        "    all_tokens_L0.append(tokens)\n",
        "\n",
        "    # Run the model and store the activations\n",
        "    _, cache = base_model.run_with_cache(tokens, stop_at_layer=layer_num + 1, \\\n",
        "                                         names_filter=[sae_id])  # [N_BATCH, N_CONTEXT, D_MODEL]\n",
        "\n",
        "    # Get the activations from the cache at the sae_id\n",
        "    activations_original = cache[sae_id]\n",
        "    # activations_filtered = filter_activations(activations_original)\n",
        "\n",
        "    # Encode the activations with the SAE\n",
        "    feature_activations = sae.encode_standard(activations_original) # the result of the encode method of the sae on the \"sae_id\" activations (a specific activation tensor of the LLM)\n",
        "    # feature_activations.to('cpu')\n",
        "\n",
        "    # Store the encoded activations\n",
        "    all_L0.append(L0_loss(feature_activations))\n",
        "\n",
        "    # Explicitly free up memory by deleting the cache and emptying the CUDA cache\n",
        "    del cache\n",
        "    del activations_original\n",
        "    del feature_activations\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Concatenate all tokens into a single tensor for reuse\n",
        "set_tokens_sample(Experiment.L0_LOSS, torch.cat(all_tokens_L0))  # [TOTAL_BATCHES * N_BATCH, N_CONTEXT]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ipq3dCzUaYqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88628303-2286-4720-9e6a-60d04c78d22e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(83.3686)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "torch.tensor(all_L0).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqZCKBJOaYqc"
      },
      "source": [
        "#### 4.1.2 Substitution Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4FMPdbmaYqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9613e63-ba02-447b-c1ed-a4f5860b5708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 50/50 [04:23<00:00,  5.26s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from torcheval.metrics import R2Score\n",
        "sae_reconstruction_metric = R2Score().to(device)\n",
        "\n",
        "all_SL_clean = []\n",
        "all_SL_reconstructed = []\n",
        "\n",
        "total_batches = get_batch_size(Experiment.SUBSTITUTION_LOSS)\n",
        "all_tokens_SL = get_tokens_sample(Experiment.SUBSTITUTION_LOSS)\n",
        "\n",
        "for k in tqdm(range(total_batches)):\n",
        "    # Get a batch of tokens from the dataset\n",
        "    tokens = activation_store.get_batch_tokens()  # [N_BATCH, N_CONTEXT]\n",
        "    # Store tokens for later reuse\n",
        "    all_tokens_SL.append(tokens)\n",
        "\n",
        "    clean_loss, reconstructed_loss = get_substitution_loss(tokens, base_model, sae, sae_id, sae_reconstruction_metric)\n",
        "\n",
        "    all_SL_clean.append(clean_loss)\n",
        "    all_SL_reconstructed.append(reconstructed_loss)\n",
        "\n",
        "# Concatenate all tokens into a single tensor for reuse\n",
        "set_tokens_sample(Experiment.SUBSTITUTION_LOSS, torch.cat(all_tokens_SL))  # [TOTAL_BATCHES * N_BATCH, N_CONTEXT]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQZzgnxwaYqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1dbf234-6831-4ac7-c9cd-0714c4bf7952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean vs substitution loss:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.77734375, 1.9267578125)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "print('Clean vs substitution loss:')\n",
        "torch.tensor(all_SL_clean).mean().item(), torch.tensor(all_SL_reconstructed).mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn7y8XQcG16I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f262c7e-65e9-4f05-b73d-e5d5685ca72b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Varience explained by SAE: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6717534065246582"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "print('Varience explained by SAE: ')\n",
        "sae_reconstruction_metric.compute().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiFDZeZhUcp4"
      },
      "source": [
        "#### 4.1.3 Feature activations histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2OlSXpsUcp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b645eab-1e95-4c07-ffdb-ef698f4f9216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 25/25 [00:47<00:00,  1.90s/it]\n"
          ]
        }
      ],
      "source": [
        "all_feature_acts = []\n",
        "\n",
        "total_batches = get_batch_size(Experiment.FEATURE_ACTS)\n",
        "all_histogram_tokens = get_tokens_sample(Experiment.FEATURE_ACTS)\n",
        "\n",
        "for k in tqdm(range(total_batches)):\n",
        "    # Get a batch of tokens from the dataset\n",
        "    tokens = activation_store.get_batch_tokens()  # [N_BATCH, N_CONTEXT]\n",
        "    all_histogram_tokens.append(tokens)\n",
        "\n",
        "    # Run the model and store the activations\n",
        "    _, cache = base_model.run_with_cache(tokens, stop_at_layer=layer_num + 1, \\\n",
        "                                         names_filter=[sae_id])  # [N_BATCH, N_CONTEXT, D_MODEL]\n",
        "\n",
        "    # Get the activations from the cache at the sae_id\n",
        "    activations_original = cache[sae_id] # [N_BATCH, N_CONTEXT, D_SAE]\n",
        "    # activations_filtered = filter_activations(activations_original)\n",
        "\n",
        "    # Encode the activations with the SAE\n",
        "    feature_activations = sae.encode_standard(activations_original) # the result of the encode method of the sae on the \"sae_id\" activations (a specific activation tensor of the LLM)\n",
        "    feature_activations = feature_activations.to('cpu')\n",
        "\n",
        "    # Store the encoded activations\n",
        "    all_feature_acts.append(feature_activations)\n",
        "\n",
        "    # Explicitly free up memory by deleting the cache and emptying the CUDA cache\n",
        "    del cache\n",
        "    del activations_original\n",
        "    del feature_activations\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "set_tokens_sample(Experiment.FEATURE_ACTS, torch.cat(all_histogram_tokens))  # [TOTAL_BATCHES * N_BATCH, N_CONTEXT]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8SY3gnYUcp5"
      },
      "outputs": [],
      "source": [
        "all_feature_acts = torch.cat(all_feature_acts)\n",
        "plot_log10_hist(all_feature_acts, 'activations')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0l2QKm5cUcp5"
      },
      "outputs": [],
      "source": [
        "del all_feature_acts\n",
        "clear_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e8hNp3QwbSG"
      },
      "source": [
        "#### 4.1.4 Feature density histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fN-lTqMwbSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166e93a6-c0db-421e-e599-b9bbe86c4fb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 50/50 [01:59<00:00,  2.39s/it]\n"
          ]
        }
      ],
      "source": [
        "all_histogram_tokens = get_tokens_sample(Experiment.FEATURE_DENSITY)\n",
        "total_batches = get_batch_size(Experiment.FEATURE_DENSITY)\n",
        "\n",
        "total_tokens = total_batches * batch_size_tokens\n",
        "n_features = sae.cfg.d_sae\n",
        "\n",
        "density_plotter = FeatureDensityPlotter(n_features, total_tokens)\n",
        "\n",
        "for k in tqdm(range(total_batches)):\n",
        "    # Get a batch of tokens from the dataset\n",
        "    tokens = activation_store.get_batch_tokens()  # [N_BATCH, N_CONTEXT]\n",
        "    all_histogram_tokens.append(tokens)\n",
        "\n",
        "    # Run the model and store the activations\n",
        "    _, cache = base_model.run_with_cache(tokens, stop_at_layer=layer_num + 1, \\\n",
        "                                         names_filter=[sae_id])  # [N_BATCH, N_CONTEXT, D_MODEL]\n",
        "\n",
        "    # Get the activations from the cache and convert to float32 for more accurate density computation\n",
        "    activations_original = cache[sae_id].flatten(0, 1).float() # [N_BATCH, N_CONTEXT, D_SAE]\n",
        "    # activations_filtered = filter_activations(activations_original)\n",
        "\n",
        "    # Encode the activations with the SAE\n",
        "    feature_activations = sae.encode_standard(activations_original) # the result of the encode method of the sae on the \"sae_id\" activations (a specific activation tensor of the LLM)\n",
        "    feature_activations = feature_activations.to('cpu')\n",
        "\n",
        "    # Update the density histogram data\n",
        "    density_plotter.update(feature_activations)\n",
        "\n",
        "    # Explicitly free up memory by deleting the cache and emptying the CUDA cache\n",
        "    del cache\n",
        "    del activations_original\n",
        "    del feature_activations\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "set_tokens_sample(Experiment.FEATURE_DENSITY, torch.cat(all_histogram_tokens))  # [TOTAL_BATCHES * N_BATCH, N_CONTEXT]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uokP6-AwbSG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "6d01887c-fb52-4d99-d42e-cb6b770fc4e6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"c19d8151-9c9d-499b-9592-344f82e124d4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c19d8151-9c9d-499b-9592-344f82e124d4\")) {                    Plotly.newPlot(                        \"c19d8151-9c9d-499b-9592-344f82e124d4\",                        [{\"width\":0.09833729594945907,\"x\":[-10.0,-9.901663,-9.803326,-9.7049885,-9.60665,-9.508313,-9.409976,-9.311639,-9.213302,-9.1149645,-9.016627,-8.91829,-8.819952,-8.721615,-8.623278,-8.5249405,-8.426603,-8.328266,-8.229929,-8.131591,-8.033254,-7.9349165,-7.8365793,-7.738242,-7.639905,-7.5415673,-7.44323,-7.344893,-7.246556,-7.148218,-7.049881,-6.951544,-6.8532066,-6.754869,-6.656532,-6.5581946,-6.459857,-6.36152,-6.2631826,-6.1648455,-6.066508,-5.9681706,-5.8698335,-5.7714963,-5.6731586,-5.5748215,-5.4764843,-5.378147,-5.2798095,-5.1814723,-5.0831356,-4.984798,-4.886461,-4.7881236,-4.6897864,-4.591449,-4.4931116,-4.3947744,-4.296437,-4.1980996,-4.0997624,-4.0014253,-3.9030879,-3.8047504,-3.7064133,-3.6080759,-3.5097387,-3.4114013,-3.313064,-3.2147267,-3.1163895,-3.018052,-2.919715,-2.8213775,-2.72304,-2.624703,-2.5263655,-2.4280283,-2.329691,-2.2313538,-2.1330163,-2.0346792,-1.9363418,-1.8380045,-1.7396672,-1.6413299,-1.5429926,-1.4446553,-1.346318,-1.2479807,-1.1496434,-1.0513061,-0.95296884,-0.8546315,-0.7562942,-0.6579569,-0.5596196,-0.4612823,-0.36294502,-0.2646077],\"y\":[497.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,440.0,0.0,0.0,470.0,0.0,449.0,514.0,445.0,466.0,1001.0,951.0,896.0,1799.0,1744.0,2010.0,2672.0,2663.0,2916.0,3265.0,3456.0,3606.0,3564.0,3750.0,3646.0,3609.0,3433.0,3265.0,3076.0,2587.0,2004.0,1552.0,1106.0,1310.0,1043.0,476.0,277.0,181.0,118.0,82.0,66.0,37.0,39.0,19.0,9.0,8.0,8.0,5.0,4.0,0.0,0.0,1.0,0.0,0.0,1.0],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"yaxis\":{\"title\":{\"text\":\"Density\"},\"range\":[0,7292.0]},\"title\":{\"text\":\"SAE Features Density histogram (Dead features density: 4.97e+02)\"},\"xaxis\":{\"title\":{\"text\":\"Log10 of Density\"}},\"bargap\":0.2,\"bargroupgap\":0.1,\"annotations\":[{\"bgcolor\":\"white\",\"bordercolor\":\"black\",\"borderwidth\":1,\"font\":{\"color\":\"red\",\"size\":12},\"showarrow\":false,\"text\":\"Dead features density: 4.97e+02\",\"x\":0.95,\"xref\":\"paper\",\"y\":0.95,\"yref\":\"paper\"}]},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c19d8151-9c9d-499b-9592-344f82e124d4');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "density_plotter.plot(y_scalar=2, y_scale_bin=-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfD3GAfmzzey"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "# Save the computed feature densities\n",
        "base_feature_densities = density_plotter.feature_densities\n",
        "\n",
        "# Choose saving names consistent with saetuning/get_scores.py\n",
        "saving_name_base = BASE_MODEL if \"/\" not in BASE_MODEL else BASE_MODEL.split(\"/\")[-1]\n",
        "saving_name_ft = FINETUNE_MODEL if \"/\" not in FINETUNE_MODEL else FINETUNE_MODEL.split(\"/\")[-1]\n",
        "saving_name_ds = DATASET_NAME if \"/\" not in DATASET_NAME else DATASET_NAME.split(\"/\")[-1]\n",
        "\n",
        "base_feature_densities_fname = f'Feature_densities_{saving_name_base}_on_{saving_name_ds}.pt'\n",
        "\n",
        "if IN_COLAB:\n",
        "    datapath = Path('/content/drive/My Drive/sae_data')\n",
        "else:\n",
        "    from saetuning.utils import get_env_var\n",
        "    _, datapath = get_env_var()\n",
        "\n",
        "saving_path = datapath / base_feature_densities_fname\n",
        "torch.save(base_feature_densities, saving_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2nk6fv5VN4I"
      },
      "outputs": [],
      "source": [
        "del base_model, activation_store\n",
        "clear_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAZhdorjaYqc"
      },
      "source": [
        "### Task 4.2 FineTuned case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "f955803c852a4573bd162f6dc9334689",
            "4fe56f014b58499eb159ae92e4b0797b",
            "df96fc3b42e24d77a27145d978f8a3c4",
            "1f8869f6ce3b493ea4d750b34fb78f97",
            "f879a3aa9ed84f58890b2067e2dc97b7",
            "5b085a36529e4faea49b71b62241a796",
            "a40eb51096ec498d9a4b8ac82809fadf",
            "e19be95f18d046958b406cea01d48d26",
            "4feb201606fc4058a4f67b1bbb3a7b4a",
            "9ec1a6ff95864b6293d7fd74695eed86",
            "7ed7f02b23fb43439a16ee4aa7387512"
          ]
        },
        "id": "cfVBH05TaYqd",
        "outputId": "edb94c95-d8d6-4944-d2da-158dcfc5fe55"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f955803c852a4573bd162f6dc9334689"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
            "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model mistral-7b into HookedTransformer\n"
          ]
        }
      ],
      "source": [
        "# Load the finetune model and its tokenizer\n",
        "finetune_tokenizer, finetune_model = load_hf_model(FINETUNE_PATH if FINETUNE_PATH is not None else FINETUNE_MODEL,\n",
        "                                                   device=device, dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2ZpTRiYA-6D"
      },
      "outputs": [],
      "source": [
        "# import the required libraries\n",
        "from sae_lens import SAE\n",
        "\n",
        "sae_id = f'blocks.{layer_num}.hook_resid_{hook_part}' # Gemma is post,\n",
        "\n",
        "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
        "                            release = RELEASE,\n",
        "                            sae_id = sae_id,\n",
        "                            device = device\n",
        ")\n",
        "cfg_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT_zSnb7aYqd"
      },
      "source": [
        "#### 4.2.1 L0 loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBHR-a5FaYqd",
        "outputId": "5e944614-6967-47ec-a20a-89081590dfa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens count:  256000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 50/50 [00:48<00:00,  1.03it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "all_L0 = []\n",
        "\n",
        "total_batches = get_batch_size(Experiment.L0_LOSS)\n",
        "all_tokens_L0 = get_tokens_sample(Experiment.L0_LOSS)\n",
        "print(f'Tokens count: ', all_tokens_L0.numel())\n",
        "\n",
        "for k in tqdm(range(total_batches)):\n",
        "    # Use the same sample to calculate the L0 loss.\n",
        "    # Calculate the start and end indices for the current batch\n",
        "    start_idx = k * batch_size_prompts\n",
        "    end_idx = (k + 1) * batch_size_prompts\n",
        "\n",
        "    # Get the corresponding batch of tokens from all_tokens\n",
        "    tokens = all_tokens_L0[start_idx:end_idx]  # [N_BATCH, N_CONTEXT]\n",
        "\n",
        "    # Run the model and store the activations\n",
        "    _, cache = finetune_model.run_with_cache(tokens, stop_at_layer=layer_num + 1, \\\n",
        "                                         names_filter=[sae_id])  # [N_BATCH, N_CONTEXT, D_MODEL]\n",
        "\n",
        "    # Get the activations from the cache at the sae_id\n",
        "    activations_original = cache[sae_id]\n",
        "    # activations_filtered = filter_activations(activations_original)\n",
        "\n",
        "    # Encode the activations with the SAE\n",
        "    feature_activations = sae.encode_standard(activations_original) # the result of the encode method of the sae on the \"sae_id\" activations (a specific activation tensor of the LLM)\n",
        "    # feature_activations.to('cpu')\n",
        "\n",
        "    # Store the encoded activations\n",
        "    all_L0.append(L0_loss(feature_activations))\n",
        "\n",
        "    # Explicitly free up memory by deleting the cache and emptying the CUDA cache\n",
        "    del cache\n",
        "    del activations_original\n",
        "    del feature_activations\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1-JljccaYqd",
        "outputId": "2882533a-401e-4227-a6bc-599979c2eb12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(90.2234)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "torch.tensor(all_L0).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK26blEThFhx"
      },
      "outputs": [],
      "source": [
        "clear_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG8fAQpuaYqd"
      },
      "source": [
        "#### 4.2.2 Substitution Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMMYKoT7aYqd",
        "outputId": "2a63eb08-c66f-4a6c-a98a-5ac39b8ad173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens count:  256000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 50/50 [04:22<00:00,  5.25s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "sae_reconstruction_metric = R2Score().to(device)\n",
        "\n",
        "all_SL_clean = []\n",
        "all_SL_reconstructed = []\n",
        "\n",
        "total_batches = get_batch_size(Experiment.SUBSTITUTION_LOSS)\n",
        "all_tokens_SL = get_tokens_sample(Experiment.SUBSTITUTION_LOSS)\n",
        "print(f'Tokens count: ', all_tokens_SL.numel())\n",
        "\n",
        "for k in tqdm(range(total_batches)):\n",
        "    # Use the same sample to calculate the losses.\n",
        "    # Calculate the start and end indices for the current batch\n",
        "    start_idx = k * batch_size_prompts\n",
        "    end_idx = (k + 1) * batch_size_prompts\n",
        "\n",
        "    # Get the corresponding batch of tokens from all_tokens\n",
        "    tokens = all_tokens_SL[start_idx:end_idx]  # [N_BATCH, N_CONTEXT]\n",
        "\n",
        "    # Store loss\n",
        "    clean_loss, reconstructed_loss = get_substitution_loss(tokens, finetune_model, sae, sae_id, sae_reconstruction_metric)\n",
        "    all_SL_clean.append(clean_loss)\n",
        "    all_SL_reconstructed.append(reconstructed_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtXPCQafaYqd",
        "outputId": "04c0c8f6-48c8-405d-cd57-238a2098f903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean vs substitution loss:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.939453125, 2.099609375)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "print('Clean vs substitution loss:')\n",
        "torch.tensor(all_SL_clean).mean().item(), torch.tensor(all_SL_reconstructed).mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVI_D4W_G16J",
        "outputId": "37219335-b14c-40cf-e7b0-6eb24185e316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Varience explained by SAE: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5796637535095215"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "print('Varience explained by SAE: ')\n",
        "sae_reconstruction_metric.compute().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JjmtOZpaYqd",
        "outputId": "7322ad84-6744-489d-bea9-aef96a9bbada"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.sort(\n",
              "values=tensor([1.2559, 1.2891, 1.5420, 1.7178, 1.7314, 1.7549, 1.8525, 1.8604, 1.9180,\n",
              "        1.9297, 1.9688, 1.9688, 1.9697, 1.9814, 2.0000, 2.0176, 2.0195, 2.0254,\n",
              "        2.0273, 2.0332, 2.0684, 2.0918, 2.0996, 2.1035, 2.1133, 2.1270, 2.1465,\n",
              "        2.1504, 2.1523, 2.1602, 2.1641, 2.1660, 2.1719, 2.1836, 2.2109, 2.2129,\n",
              "        2.2617, 2.2656, 2.2891, 2.3359, 2.3379, 2.3496, 2.3711, 2.4004, 2.4336,\n",
              "        2.4414, 2.4473, 2.5117, 2.5176, 2.7988], dtype=torch.float16),\n",
              "indices=tensor([42,  9, 43, 30, 45, 34, 40, 29, 41,  6, 49, 39, 18, 15, 13, 16,  8,  7,\n",
              "         4, 27, 17, 28,  3, 37, 48, 44,  5,  0, 20, 10, 14, 12, 33, 11, 26, 38,\n",
              "        19, 35, 36, 22, 46, 31, 25, 24, 47, 23, 32, 21,  2,  1]))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "loss_reconstructed_tensor = torch.tensor(all_SL_reconstructed)\n",
        "loss_reconstructed_tensor.sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77SOQqGUGDgS",
        "outputId": "36ceb47b-26a2-41ec-aeac-5398820d32c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered substitution loss = 2.099609375\n"
          ]
        }
      ],
      "source": [
        "# Filter out NaN values (if there are any)\n",
        "filtered_loss_reconstructed = loss_reconstructed_tensor[~torch.isinf(loss_reconstructed_tensor)]\n",
        "print(f'Filtered substitution loss = {filtered_loss_reconstructed.mean().item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQfS5sF1IkDL"
      },
      "source": [
        "#### 4.2.3 Feature activations histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17LetYS7Ucp6"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "all_feature_acts = []\n",
        "\n",
        "total_batches = get_batch_size(Experiment.FEATURE_ACTS)\n",
        "all_histogram_tokens = get_tokens_sample(Experiment.FEATURE_ACTS)\n",
        "print(f'Tokens count: ', all_histogram_tokens.numel())\n",
        "\n",
        "for k in tqdm(range(total_batches)):\n",
        "    # Use the same sample to calculate the histogram\n",
        "    # Calculate the start and end indices for the current batch\n",
        "    start_idx = k * batch_size_prompts\n",
        "    end_idx = (k + 1) * batch_size_prompts\n",
        "\n",
        "    # Get the corresponding batch of tokens from all_tokens\n",
        "    tokens = all_histogram_tokens[start_idx:end_idx]  # [N_BATCH, N_CONTEXT]\n",
        "\n",
        "    # Run the model and store the activations\n",
        "    _, cache = finetune_model.run_with_cache(tokens, stop_at_layer=layer_num + 1, \\\n",
        "                                         names_filter=[sae_id])  # [N_BATCH, N_CONTEXT, D_MODEL]\n",
        "\n",
        "    # Get the activations from the cache at the sae_id\n",
        "    activations_original = cache[sae_id]\n",
        "    # activations_filtered = filter_activations(activations_original)\n",
        "\n",
        "    # Encode the activations with the SAE\n",
        "    feature_activations = sae.encode_standard(activations_original) # the result of the encode method of the sae on the \"sae_id\" activations (a specific activation tensor of the LLM)\n",
        "    feature_activations = feature_activations.to('cpu')\n",
        "\n",
        "    # Store the encoded activations\n",
        "    all_feature_acts.append(feature_activations)\n",
        "\n",
        "    # Explicitly free up memory by deleting the cache and emptying the CUDA cache\n",
        "    del cache\n",
        "    del activations_original\n",
        "    del feature_activations\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMVZ8NelUcp6"
      },
      "outputs": [],
      "source": [
        "all_feature_acts = torch.cat(all_feature_acts)\n",
        "plot_log10_hist(all_feature_acts, 'activations')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del all_feature_acts\n",
        "clear_cache()"
      ],
      "metadata": {
        "id": "b9qdRC9kp5St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmvgyCgWwbSI"
      },
      "source": [
        "#### 4.2.4 Feature density histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbD4e4vDwbSI",
        "outputId": "ba50e5b5-ed8a-4a74-be93-dc40ff8fa84a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens count:  256000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 50/50 [01:59<00:00,  2.39s/it]\n"
          ]
        }
      ],
      "source": [
        "all_histogram_tokens = get_tokens_sample(Experiment.FEATURE_DENSITY)\n",
        "total_batches = get_batch_size(Experiment.FEATURE_DENSITY)\n",
        "print(f'Tokens count: ', all_histogram_tokens.numel())\n",
        "\n",
        "total_tokens = total_batches * batch_size_tokens\n",
        "n_features = sae.cfg.d_sae\n",
        "\n",
        "density_plotter = FeatureDensityPlotter(n_features, total_tokens)\n",
        "\n",
        "for k in tqdm(range(total_batches)):\n",
        "    # Use the same sample to calculate the histogram\n",
        "    # Calculate the start and end indices for the current batch\n",
        "    start_idx = k * batch_size_prompts\n",
        "    end_idx = (k + 1) * batch_size_prompts\n",
        "\n",
        "    # Get the corresponding batch of tokens from all_tokens\n",
        "    tokens = all_histogram_tokens[start_idx:end_idx]  # [N_BATCH, N_CONTEXT]\n",
        "\n",
        "    # Run the model and store the activations\n",
        "    _, cache = finetune_model.run_with_cache(tokens, stop_at_layer=layer_num + 1, \\\n",
        "                                             names_filter=[sae_id])  # [N_BATCH, N_CONTEXT, D_MODEL]\n",
        "\n",
        "    # Get the activations from the cache at the sae_id\n",
        "    activations_original = cache[sae_id].flatten(0, 1).float()\n",
        "    # activations_filtered = filter_activations(activations_original)\n",
        "\n",
        "    # Encode the activations with the SAE\n",
        "    feature_activations = sae.encode_standard(activations_original) # the result of the encode method of the sae on the \"sae_id\" activations (a specific activation tensor of the LLM)\n",
        "    feature_activations = feature_activations.to('cpu')\n",
        "\n",
        "    # Update the density histogram data\n",
        "    density_plotter.update(feature_activations)\n",
        "\n",
        "    # Explicitly free up memory by deleting the cache and emptying the CUDA cache\n",
        "    del cache\n",
        "    del activations_original\n",
        "    del feature_activations\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "leqgFkTKwbSI",
        "outputId": "202d4b53-b12f-49d0-f0e2-044183335d4a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"201e9c79-0291-48a5-b97c-31420ecb428e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"201e9c79-0291-48a5-b97c-31420ecb428e\")) {                    Plotly.newPlot(                        \"201e9c79-0291-48a5-b97c-31420ecb428e\",                        [{\"width\":0.09665364861488342,\"x\":[-10.0,-9.903346,-9.806693,-9.710039,-9.613385,-9.516731,-9.420078,-9.323424,-9.22677,-9.130117,-9.0334635,-8.93681,-8.840157,-8.743503,-8.646849,-8.550195,-8.453542,-8.356888,-8.260234,-8.163581,-8.066927,-7.970273,-7.8736196,-7.776966,-7.680312,-7.5836587,-7.4870048,-7.3903513,-7.293698,-7.197044,-7.1003904,-7.0037365,-6.907083,-6.8104296,-6.7137756,-6.617122,-6.520468,-6.423815,-6.3271613,-6.2305074,-6.133854,-6.0372,-5.9405465,-5.843893,-5.747239,-5.6505857,-5.5539317,-5.4572783,-5.3606243,-5.263971,-5.167318,-5.0706644,-4.9740105,-4.877357,-4.780703,-4.6840496,-4.587396,-4.490742,-4.3940887,-4.297435,-4.2007813,-4.104128,-4.007474,-3.9108205,-3.8141668,-3.717513,-3.6208594,-3.5242057,-3.4275522,-3.3308985,-3.2342448,-3.1375911,-3.0409374,-2.9442837,-2.8476303,-2.7509766,-2.6543229,-2.5576692,-2.4610155,-2.364362,-2.2677083,-2.1710546,-2.074401,-1.9777473,-1.8810936,-1.7844399,-1.6877863,-1.5911326,-1.494479,-1.3978254,-1.3011717,-1.2045181,-1.1078644,-1.0112107,-0.9145571,-0.8179034,-0.72124976,-0.6245961,-0.5279424,-0.43128878],\"y\":[419.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,298.0,0.0,0.0,336.0,0.0,358.0,423.0,405.0,423.0,858.0,836.0,855.0,1598.0,1520.0,1921.0,2165.0,2655.0,2933.0,3053.0,3342.0,3575.0,3728.0,3601.0,3678.0,3649.0,3516.0,3258.0,3129.0,2880.0,2286.0,1824.0,1384.0,1177.0,1523.0,699.0,439.0,271.0,146.0,114.0,79.0,54.0,46.0,31.0,17.0,14.0,5.0,4.0,4.0,4.0,1.0,0.0,0.0,2.0],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"yaxis\":{\"title\":{\"text\":\"Density\"},\"range\":[0,5592.0]},\"title\":{\"text\":\"SAE Features Density histogram (Dead features density: 4.19e+02)\"},\"xaxis\":{\"title\":{\"text\":\"Log10 of Density\"}},\"bargap\":0.2,\"bargroupgap\":0.1,\"annotations\":[{\"bgcolor\":\"white\",\"bordercolor\":\"black\",\"borderwidth\":1,\"font\":{\"color\":\"red\",\"size\":12},\"showarrow\":false,\"text\":\"Dead features density: 4.19e+02\",\"x\":0.95,\"xref\":\"paper\",\"y\":0.95,\"yref\":\"paper\"}]},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('201e9c79-0291-48a5-b97c-31420ecb428e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "density_plotter.plot(y_scalar=1.5, y_scale_bin=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lop2WUJWzze3"
      },
      "outputs": [],
      "source": [
        "# Save the computed feature densities\n",
        "finetune_feature_densities = density_plotter.feature_densities\n",
        "\n",
        "# Choose saving names consistent with saetuning/get_scores.py\n",
        "finetune_feature_densities_fname = f'Feature_densities_{saving_name_ft}_on_{saving_name_ds}.pt'\n",
        "\n",
        "if IN_COLAB:\n",
        "    datapath = Path('/content/drive/My Drive/sae_data')\n",
        "else:\n",
        "    from saetuning.utils import get_env_var\n",
        "    _, datapath = get_env_var()\n",
        "\n",
        "saving_path = datapath / finetune_feature_densities_fname\n",
        "torch.save(finetune_feature_densities, saving_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpGKy6iPwbSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e8f70d2-c289-4470-e00c-ee9f7f1345d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256000"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "total_tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UYfRit0t5Dj-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2d12eb0ab6ae49a7857b9bd33886edd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f390c934f4f4df6b175c00fad546ac4",
              "IPY_MODEL_16f28c24722f4a45989928a2f7b87107",
              "IPY_MODEL_772465b9d19041a196494f0424756f6d"
            ],
            "layout": "IPY_MODEL_daaf163e20d942968e37a033db88bbe8"
          }
        },
        "6f390c934f4f4df6b175c00fad546ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e65f57af640f4db9819bb1561579d42f",
            "placeholder": "",
            "style": "IPY_MODEL_c66930b0cb43498dbfd9340338ef0ca2",
            "value": "Loadingcheckpointshards:100%"
          }
        },
        "16f28c24722f4a45989928a2f7b87107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6d3c329a9b9446ebca217bf4301c04e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44d805d0088d4d2eba4e25d90fa55f6d",
            "value": 2
          }
        },
        "772465b9d19041a196494f0424756f6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35f4620e37ff47d4a540798210439ec0",
            "placeholder": "",
            "style": "IPY_MODEL_ee3a337275cb42218137525c046e053a",
            "value": "2/2[01:02&lt;00:00,28.96s/it]"
          }
        },
        "daaf163e20d942968e37a033db88bbe8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e65f57af640f4db9819bb1561579d42f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c66930b0cb43498dbfd9340338ef0ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6d3c329a9b9446ebca217bf4301c04e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44d805d0088d4d2eba4e25d90fa55f6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35f4620e37ff47d4a540798210439ec0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee3a337275cb42218137525c046e053a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a851a7c7f2634655a2f64b0e9de01028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87d2a1cd47ae429c85992ad50b19f410",
              "IPY_MODEL_af7fdc7b78a04aa9990f6de345eb22db",
              "IPY_MODEL_8f94fc386e5d4173a8b90b0d9f396d0e"
            ],
            "layout": "IPY_MODEL_a67ec4750eb4472dbebf43354af4c58e"
          }
        },
        "87d2a1cd47ae429c85992ad50b19f410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac67f2d4794641e0915469d9df8b8085",
            "placeholder": "",
            "style": "IPY_MODEL_54348c8d3d604739a84cb1d31c7a9b88",
            "value": "Resolvingdatafiles:100%"
          }
        },
        "af7fdc7b78a04aa9990f6de345eb22db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41ad50fff1b94f2c920ba9c040f0ed5d",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6e20ad247654f9d86fb30c2b304e87d",
            "value": 30
          }
        },
        "8f94fc386e5d4173a8b90b0d9f396d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4632488209fd4ec6a07ea343c0abe900",
            "placeholder": "",
            "style": "IPY_MODEL_ea8074c65241405b8adc334684d6ddc2",
            "value": "30/30[00:00&lt;00:00,7.58it/s]"
          }
        },
        "a67ec4750eb4472dbebf43354af4c58e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac67f2d4794641e0915469d9df8b8085": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54348c8d3d604739a84cb1d31c7a9b88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41ad50fff1b94f2c920ba9c040f0ed5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6e20ad247654f9d86fb30c2b304e87d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4632488209fd4ec6a07ea343c0abe900": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea8074c65241405b8adc334684d6ddc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f955803c852a4573bd162f6dc9334689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4fe56f014b58499eb159ae92e4b0797b",
              "IPY_MODEL_df96fc3b42e24d77a27145d978f8a3c4",
              "IPY_MODEL_1f8869f6ce3b493ea4d750b34fb78f97"
            ],
            "layout": "IPY_MODEL_f879a3aa9ed84f58890b2067e2dc97b7"
          }
        },
        "4fe56f014b58499eb159ae92e4b0797b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b085a36529e4faea49b71b62241a796",
            "placeholder": "",
            "style": "IPY_MODEL_a40eb51096ec498d9a4b8ac82809fadf",
            "value": "Loadingcheckpointshards:100%"
          }
        },
        "df96fc3b42e24d77a27145d978f8a3c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e19be95f18d046958b406cea01d48d26",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4feb201606fc4058a4f67b1bbb3a7b4a",
            "value": 6
          }
        },
        "1f8869f6ce3b493ea4d750b34fb78f97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ec1a6ff95864b6293d7fd74695eed86",
            "placeholder": "",
            "style": "IPY_MODEL_7ed7f02b23fb43439a16ee4aa7387512",
            "value": "6/6[02:50&lt;00:00,26.06s/it]"
          }
        },
        "f879a3aa9ed84f58890b2067e2dc97b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b085a36529e4faea49b71b62241a796": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a40eb51096ec498d9a4b8ac82809fadf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e19be95f18d046958b406cea01d48d26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4feb201606fc4058a4f67b1bbb3a7b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ec1a6ff95864b6293d7fd74695eed86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ed7f02b23fb43439a16ee4aa7387512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}